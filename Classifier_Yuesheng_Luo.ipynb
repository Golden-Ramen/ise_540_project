{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\luoyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\luoyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "<ipython-input-1-2bbd596b2b2d>:13: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import emoji\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from langdetect import detect\n",
    "nltk.download('punkt')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "nltk.download('vader_lexicon')\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "sentiment_df = pd.read_csv(r\"C:\\Users\\luoyu\\Desktop\\ise540\\project\\full_tweet_sample.csv\")\n",
    "sample_again = pd.read_csv(r\"C:\\Users\\luoyu\\Desktop\\ise540\\project\\sample_again.csv\")\n",
    "sample_again = sample_again.rename(columns={'full_txt':'full_text'})\n",
    "sentiment_df.drop(columns=['tweet_id'])\n",
    "sample_again.drop(columns=['groupcount'])\n",
    "sentiment_df = pd.concat([sentiment_df,sample_again])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_txt = list(sentiment_df[\"full_text\"])\n",
    "true_sent = list(sentiment_df[\"sentiment\"])\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_sent = [0 if x==1 else x for x in true_sent]\n",
    "true_sent = [0 if x==10 else x for x in true_sent]\n",
    "#true_sent = [1 if x==-1 else x for x in true_sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Clssifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(list_of_txt, true_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 500)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x),len(valid_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding sentiment  analysis feature with textblob and vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_sentiment(text_list,method):\n",
    "    textblob_vader = []\n",
    "#     vader_list = []\n",
    "    for each_tweet in text_list:\n",
    "        blob=TextBlob(each_tweet)  #error: should be str not class<'float'>\n",
    "        textblob_senti=blob.sentiment.polarity\n",
    "        vader_senti=sid.polarity_scores(each_tweet)\n",
    "        if method == \"vader\":\n",
    "            textblob_vader.append(vader_senti['compound'])\n",
    "        elif method == \"textblob\":\n",
    "            textblob_vader.append(textblob_senti)\n",
    "        \n",
    "    return textblob_vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_textblob = get_sentiment(train_x,\"textblob\")\n",
    "train_vader = get_sentiment(train_x,\"vader\")\n",
    "valid_textblob = get_sentiment(valid_x,\"textblob\")\n",
    "valid_vader = get_sentiment(valid_x,\"vader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,4), max_features=6000)\n",
    "tfidf_vect_ngram.fit(list_of_txt)\n",
    "xtrain_tfidf_ngram = tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram = tfidf_vect_ngram.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_textblob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-db5a138418ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#coo_matrix([[x] for x in train_textblob])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mxtrain_tfidf_ngram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mxtrain_tfidf_ngram\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_textblob\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mxtrain_tfidf_ngram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mxtrain_tfidf_ngram\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_vader\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mxvalid_tfidf_ngram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mxvalid_tfidf_ngram\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalid_textblob\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mxvalid_tfidf_ngram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mxvalid_tfidf_ngram\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalid_vader\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_textblob' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#coo_matrix([[x] for x in train_textblob])\n",
    "xtrain_tfidf_ngram = csr_matrix(hstack([xtrain_tfidf_ngram,coo_matrix([[100*(x+1)] for x in train_textblob])]))\n",
    "xtrain_tfidf_ngram = csr_matrix(hstack([xtrain_tfidf_ngram,coo_matrix([[100*(x+1)] for x in train_vader])]))\n",
    "xvalid_tfidf_ngram = csr_matrix(hstack([xvalid_tfidf_ngram,coo_matrix([[100*(x+1)] for x in valid_textblob])]))\n",
    "xvalid_tfidf_ngram = csr_matrix(hstack([xvalid_tfidf_ngram,coo_matrix([[100*(x+1)] for x in valid_vader])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1500x6000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 28614 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_tfidf_ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\luoyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "<ipython-input-18-dbaae962a708>:7: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  w2v = dict(zip(model.wv.index2word, model.wv.syn0))\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.downloader\n",
    "nltk.download('vader_lexicon')\n",
    "#split sentence to word tokens\n",
    "tokens = [nltk.word_tokenize(sentences) for sentences in list_of_txt]\n",
    "model = gensim.models.Word2Vec(tokens, min_count=1, size=200) \n",
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-71df2232aaf5>:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  glove_vectors_w2v = dict(zip(glove_vectors.wv.index2word, glove_vectors.wv.syn0))\n",
      "<ipython-input-19-71df2232aaf5>:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  glove_vectors_w2v = dict(zip(glove_vectors.wv.index2word, glove_vectors.wv.syn0))\n"
     ]
    }
   ],
   "source": [
    "glove_vectors = gensim.downloader.load('glove-twitter-200')\n",
    "glove_vectors_w2v = dict(zip(glove_vectors.wv.index2word, glove_vectors.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.update(glove_vectors_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_model = TfidfVectorizer()\n",
    "Tfidf_model.fit(list_of_txt)\n",
    "tf_idf_dict = dict(zip(Tfidf_model.get_feature_names(), list(Tfidf_model.idf_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def embedding(list_of_txt):\n",
    "    Tfidf_model = TfidfVectorizer()\n",
    "    Tfidf_model.fit(list_of_txt)\n",
    "    tf_idf_dict = dict(zip(Tfidf_model.get_feature_names(), list(Tfidf_model.idf_)))\n",
    "    documents = []\n",
    "    for count,tweet in enumerate(list_of_txt):\n",
    "    #word_vectors = []\n",
    "        weight_sum = 0\n",
    "        for word in tokens[count]: # or your logic for separating tokens\n",
    "            sent_vec = np.zeros(200)\n",
    "            if word in tf_idf_dict:\n",
    "                tf_idf_score = tf_idf_dict[word]\n",
    "                sent_vec += w2v[word]*tf_idf_score\n",
    "                weight_sum += tf_idf_score\n",
    "                #word_vectors.append(sent_vec)\n",
    "        if weight_sum != 0:\n",
    "            sent_vec /= weight_sum\n",
    "        documents.append(sent_vec)\n",
    "    return documents \n",
    "        #documents.append(np.concatenate(word_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = embedding(list_of_txt)\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(csr_matrix(documents), true_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------The report of KNN --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.43      0.24      0.31       103\n",
      "           0       0.82      0.92      0.87       397\n",
      "\n",
      "    accuracy                           0.78       500\n",
      "   macro avg       0.63      0.58      0.59       500\n",
      "weighted avg       0.74      0.78      0.75       500\n",
      "\n",
      "    -1    0\n",
      "-1  25  78 \n",
      " 0  33  364\n",
      "------------------The report of DT --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.34      0.37      0.35       103\n",
      "           0       0.83      0.81      0.82       397\n",
      "\n",
      "    accuracy                           0.72       500\n",
      "   macro avg       0.59      0.59      0.59       500\n",
      "weighted avg       0.73      0.72      0.73       500\n",
      "\n",
      "    -1    0\n",
      "-1  38  65 \n",
      " 0  74  323\n",
      "------------------The report of SV --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00       103\n",
      "           0       0.79      0.99      0.88       397\n",
      "\n",
      "    accuracy                           0.79       500\n",
      "   macro avg       0.40      0.50      0.44       500\n",
      "weighted avg       0.63      0.79      0.70       500\n",
      "\n",
      "    -1    0\n",
      "-1  0   103\n",
      " 0  2   395\n",
      "------------------The report of GB --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.42      0.13      0.19       103\n",
      "           0       0.81      0.95      0.88       397\n",
      "\n",
      "    accuracy                           0.78       500\n",
      "   macro avg       0.61      0.54      0.53       500\n",
      "weighted avg       0.73      0.78      0.73       500\n",
      "\n",
      "    -1    0\n",
      "-1  13  90 \n",
      " 0  18  379\n",
      "------------------The report of RF --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.34      0.18      0.24       103\n",
      "           0       0.81      0.91      0.86       397\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.58      0.55      0.55       500\n",
      "weighted avg       0.71      0.76      0.73       500\n",
      "\n",
      "    -1    0\n",
      "-1  19  84 \n",
      " 0  37  360\n",
      "[23:07:11] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "------------------The report of XGB --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.17      0.02      0.03       103\n",
      "           0       0.79      0.97      0.87       397\n",
      "\n",
      "    accuracy                           0.78       500\n",
      "   macro avg       0.48      0.50      0.45       500\n",
      "weighted avg       0.66      0.78      0.70       500\n",
      "\n",
      "    -1    0\n",
      "-1  2   101\n",
      " 0  10  387\n",
      "------------------The report of KNN --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.38      0.28      0.32        95\n",
      "           0       0.84      0.89      0.86       405\n",
      "\n",
      "    accuracy                           0.77       500\n",
      "   macro avg       0.61      0.59      0.59       500\n",
      "weighted avg       0.75      0.77      0.76       500\n",
      "\n",
      "    -1    0\n",
      "-1  27  68 \n",
      " 0  45  360\n",
      "------------------The report of DT --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.27      0.41      0.32        95\n",
      "           0       0.84      0.74      0.79       405\n",
      "\n",
      "    accuracy                           0.68       500\n",
      "   macro avg       0.56      0.57      0.56       500\n",
      "weighted avg       0.73      0.68      0.70       500\n",
      "\n",
      "     -1    0\n",
      "-1  39   56 \n",
      " 0  106  299\n",
      "------------------The report of SV --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.38      0.03      0.06        95\n",
      "           0       0.81      0.99      0.89       405\n",
      "\n",
      "    accuracy                           0.81       500\n",
      "   macro avg       0.59      0.51      0.48       500\n",
      "weighted avg       0.73      0.81      0.73       500\n",
      "\n",
      "    -1    0\n",
      "-1  3   92 \n",
      " 0  5   400\n",
      "------------------The report of GB --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.35      0.15      0.21        95\n",
      "           0       0.82      0.94      0.88       405\n",
      "\n",
      "    accuracy                           0.79       500\n",
      "   macro avg       0.59      0.54      0.54       500\n",
      "weighted avg       0.73      0.79      0.75       500\n",
      "\n",
      "    -1    0\n",
      "-1  14  81 \n",
      " 0  26  379\n",
      "------------------The report of RF --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.37      0.27      0.31        95\n",
      "           0       0.84      0.89      0.86       405\n",
      "\n",
      "    accuracy                           0.77       500\n",
      "   macro avg       0.60      0.58      0.59       500\n",
      "weighted avg       0.75      0.77      0.76       500\n",
      "\n",
      "    -1    0\n",
      "-1  26  69 \n",
      " 0  45  360\n",
      "[23:07:22] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "------------------The report of XGB --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.36      0.04      0.08        95\n",
      "           0       0.81      0.98      0.89       405\n",
      "\n",
      "    accuracy                           0.80       500\n",
      "   macro avg       0.59      0.51      0.48       500\n",
      "weighted avg       0.73      0.80      0.74       500\n",
      "\n",
      "    -1    0\n",
      "-1  4   91 \n",
      " 0  7   398\n",
      "------------------The report of KNN --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.52      0.26      0.34       121\n",
      "           0       0.80      0.92      0.85       379\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.66      0.59      0.60       500\n",
      "weighted avg       0.73      0.76      0.73       500\n",
      "\n",
      "    -1    0\n",
      "-1  31  90 \n",
      " 0  29  350\n",
      "------------------The report of DT --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.30      0.30      0.30       121\n",
      "           0       0.78      0.78      0.78       379\n",
      "\n",
      "    accuracy                           0.66       500\n",
      "   macro avg       0.54      0.54      0.54       500\n",
      "weighted avg       0.66      0.66      0.66       500\n",
      "\n",
      "    -1    0\n",
      "-1  36  85 \n",
      " 0  84  295\n",
      "------------------The report of SV --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.02      0.03       121\n",
      "           0       0.76      1.00      0.86       379\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.88      0.51      0.45       500\n",
      "weighted avg       0.82      0.76      0.66       500\n",
      "\n",
      "    -1    0\n",
      "-1  2   119\n",
      " 0  0   379\n",
      "------------------The report of GB --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.78      0.15      0.25       121\n",
      "           0       0.78      0.99      0.87       379\n",
      "\n",
      "    accuracy                           0.78       500\n",
      "   macro avg       0.78      0.57      0.56       500\n",
      "weighted avg       0.78      0.78      0.72       500\n",
      "\n",
      "    -1    0\n",
      "-1  18  103\n",
      " 0  5   374\n",
      "------------------The report of RF --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.44      0.23      0.30       121\n",
      "           0       0.79      0.91      0.84       379\n",
      "\n",
      "    accuracy                           0.74       500\n",
      "   macro avg       0.62      0.57      0.57       500\n",
      "weighted avg       0.70      0.74      0.71       500\n",
      "\n",
      "    -1    0\n",
      "-1  28  93 \n",
      " 0  35  344\n",
      "[23:07:33] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------The report of XGB --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.40      0.02      0.03       121\n",
      "           0       0.76      0.99      0.86       379\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.58      0.50      0.45       500\n",
      "weighted avg       0.67      0.76      0.66       500\n",
      "\n",
      "    -1    0\n",
      "-1  2   119\n",
      " 0  3   376\n",
      "------------------The report of KNN --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.46      0.23      0.31       115\n",
      "           0       0.80      0.92      0.85       385\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.63      0.58      0.58       500\n",
      "weighted avg       0.72      0.76      0.73       500\n",
      "\n",
      "    -1    0\n",
      "-1  27  88 \n",
      " 0  32  353\n",
      "------------------The report of DT --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.35      0.38      0.37       115\n",
      "           0       0.81      0.79      0.80       385\n",
      "\n",
      "    accuracy                           0.69       500\n",
      "   macro avg       0.58      0.58      0.58       500\n",
      "weighted avg       0.70      0.69      0.70       500\n",
      "\n",
      "    -1    0\n",
      "-1  44  71 \n",
      " 0  82  303\n",
      "------------------The report of SV --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.50      0.02      0.03       115\n",
      "           0       0.77      0.99      0.87       385\n",
      "\n",
      "    accuracy                           0.77       500\n",
      "   macro avg       0.64      0.51      0.45       500\n",
      "weighted avg       0.71      0.77      0.68       500\n",
      "\n",
      "    -1    0\n",
      "-1  2   113\n",
      " 0  2   383\n",
      "------------------The report of GB --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.50      0.14      0.22       115\n",
      "           0       0.79      0.96      0.87       385\n",
      "\n",
      "    accuracy                           0.77       500\n",
      "   macro avg       0.64      0.55      0.54       500\n",
      "weighted avg       0.72      0.77      0.72       500\n",
      "\n",
      "    -1    0\n",
      "-1  16  99 \n",
      " 0  16  369\n",
      "------------------The report of RF --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.34      0.17      0.23       115\n",
      "           0       0.79      0.90      0.84       385\n",
      "\n",
      "    accuracy                           0.73       500\n",
      "   macro avg       0.56      0.54      0.54       500\n",
      "weighted avg       0.68      0.73      0.70       500\n",
      "\n",
      "    -1    0\n",
      "-1  20  95 \n",
      " 0  38  347\n",
      "[23:07:44] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "------------------The report of XGB --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.60      0.03      0.05       115\n",
      "           0       0.77      0.99      0.87       385\n",
      "\n",
      "    accuracy                           0.77       500\n",
      "   macro avg       0.69      0.51      0.46       500\n",
      "weighted avg       0.73      0.77      0.68       500\n",
      "\n",
      "    -1    0\n",
      "-1  3   112\n",
      " 0  2   383\n",
      "------------------The report of KNN --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.48      0.23      0.32       115\n",
      "           0       0.80      0.92      0.86       385\n",
      "\n",
      "    accuracy                           0.77       500\n",
      "   macro avg       0.64      0.58      0.59       500\n",
      "weighted avg       0.73      0.77      0.73       500\n",
      "\n",
      "    -1    0\n",
      "-1  27  88 \n",
      " 0  29  356\n",
      "------------------The report of DT --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.29      0.30      0.30       115\n",
      "           0       0.79      0.78      0.78       385\n",
      "\n",
      "    accuracy                           0.67       500\n",
      "   macro avg       0.54      0.54      0.54       500\n",
      "weighted avg       0.67      0.67      0.67       500\n",
      "\n",
      "    -1    0\n",
      "-1  35  80 \n",
      " 0  85  300\n",
      "------------------The report of SV --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.43      0.03      0.05       115\n",
      "           0       0.77      0.99      0.87       385\n",
      "\n",
      "    accuracy                           0.77       500\n",
      "   macro avg       0.60      0.51      0.46       500\n",
      "weighted avg       0.69      0.77      0.68       500\n",
      "\n",
      "    -1    0\n",
      "-1  3   112\n",
      " 0  4   381\n",
      "------------------The report of GB --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.53      0.15      0.23       115\n",
      "           0       0.79      0.96      0.87       385\n",
      "\n",
      "    accuracy                           0.77       500\n",
      "   macro avg       0.66      0.55      0.55       500\n",
      "weighted avg       0.73      0.77      0.72       500\n",
      "\n",
      "    -1    0\n",
      "-1  17  98 \n",
      " 0  15  370\n",
      "------------------The report of RF --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.37      0.22      0.27       115\n",
      "           0       0.79      0.89      0.84       385\n",
      "\n",
      "    accuracy                           0.74       500\n",
      "   macro avg       0.58      0.55      0.56       500\n",
      "weighted avg       0.70      0.74      0.71       500\n",
      "\n",
      "    -1    0\n",
      "-1  25  90 \n",
      " 0  42  343\n",
      "[23:07:56] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "------------------The report of XGB --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.67      0.03      0.07       115\n",
      "           0       0.78      0.99      0.87       385\n",
      "\n",
      "    accuracy                           0.77       500\n",
      "   macro avg       0.72      0.51      0.47       500\n",
      "weighted avg       0.75      0.77      0.69       500\n",
      "\n",
      "    -1    0\n",
      "-1  4   111\n",
      " 0  2   383\n"
     ]
    }
   ],
   "source": [
    "def splitandtrain(documents,true_sent):\n",
    "    train_x, valid_x, train_y, valid_y = model_selection.train_test_split(csr_matrix(documents), true_sent)\n",
    "    report_list = train_classifier(train_x, train_y, valid_x,valid_y)\n",
    "    return report_list\n",
    "reports = []\n",
    "for i in range(5):\n",
    "    reports.append(splitandtrain(documents,true_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_classifier() missing 1 required positional argument: 'valid_y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-d6e9704bf98c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: train_classifier() missing 1 required positional argument: 'valid_y'"
     ]
    }
   ],
   "source": [
    "train_classifier(train_x, train_y, valid_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     7
    ]
   },
   "outputs": [],
   "source": [
    "MN = naive_bayes.MultinomialNB()\n",
    "KNN = KNeighborsClassifier()\n",
    "LR = linear_model.LogisticRegression()\n",
    "DT = tree.DecisionTreeClassifier()\n",
    "SV = SVC(kernel='rbf', probability=True)\n",
    "GB = GradientBoostingClassifier(random_state=0)\n",
    "RF = RandomForestClassifier()\n",
    "XGB = xgb_model = xgb.XGBClassifier(base_score=0.5, booster='gblinear', colsample_bylevel=None,\n",
    "              colsample_bynode=None, colsample_bytree=1.0, gamma=5, gpu_id=-1,\n",
    "              importance_type='gain', interaction_constraints=None,\n",
    "              learning_rate=0.02, max_delta_step=None, max_depth=5,\n",
    "              min_child_weight=5, monotone_constraints=None,\n",
    "              n_estimators=100, n_jobs=0, num_parallel_tree=None,\n",
    "              objective='binary:hinge', random_state=0, reg_alpha=0,\n",
    "              reg_lambda=0, scale_pos_weight=None, subsample=0.6,\n",
    "              tree_method=None, validate_parameters=1, verbosity=None)\n",
    "\n",
    "test_classifiers = [ KNN, DT, SV, GB, RF, XGB]\n",
    "test_classifiers_flag = ['KNN','DT', 'SV','GB','RF', 'XGB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classifiers = [DT]\n",
    "test_classifiers_flag = ['DT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid,valid_y):\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    labels = np.unique(true_sent)\n",
    "    a =confusion_matrix(valid_y,predictions,labels = labels)\n",
    "    conf = pd.DataFrame(a, index=labels, columns=labels)\n",
    "    report = classification_report(valid_y,predictions,labels = labels)\n",
    "    \n",
    "    return metrics.f1_score(valid_y,predictions, average='weighted'),conf,report\n",
    "# metrics.accuracy_score(valid_y,predictions),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_classifier(X_train, y_train, X_valid,valid_y):\n",
    "    report_list=[]\n",
    "    for classifier, flag in zip(test_classifiers, test_classifiers_flag):\n",
    "        f1,conf,report=train_model(classifier, X_train, y_train, X_valid,valid_y)\n",
    "        print('------------------The report of',flag,'--------------------')\n",
    "        print(report)\n",
    "        print(conf)\n",
    "        report_list.append((flag,report))\n",
    "    return report_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitandtrain(list_of_txt,true_sent):\n",
    "    train_x, valid_x, train_y, valid_y = model_selection.train_test_split(list_of_txt, true_sent)\n",
    "    tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,4), max_features=6000)\n",
    "    tfidf_vect_ngram.fit(list_of_txt)\n",
    "    xtrain_tfidf_ngram = tfidf_vect_ngram.transform(train_x)\n",
    "    xvalid_tfidf_ngram = tfidf_vect_ngram.transform(valid_x)\n",
    "    report_list = train_classifier(xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram,valid_y)\n",
    "    return report_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------The report of KNN --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.45      0.19      0.27       103\n",
      "           0       0.82      0.94      0.87       397\n",
      "\n",
      "    accuracy                           0.79       500\n",
      "   macro avg       0.64      0.57      0.57       500\n",
      "weighted avg       0.74      0.79      0.75       500\n",
      "\n",
      "    -1    0\n",
      "-1  20  83 \n",
      " 0  24  373\n",
      "------------------The report of DT --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.43      0.42      0.42       103\n",
      "           0       0.85      0.85      0.85       397\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.64      0.64      0.64       500\n",
      "weighted avg       0.76      0.76      0.76       500\n",
      "\n",
      "    -1    0\n",
      "-1  43  60 \n",
      " 0  58  339\n",
      "------------------The report of SV --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.83      0.18      0.30       103\n",
      "           0       0.82      0.99      0.90       397\n",
      "\n",
      "    accuracy                           0.82       500\n",
      "   macro avg       0.82      0.59      0.60       500\n",
      "weighted avg       0.82      0.82      0.78       500\n",
      "\n",
      "    -1    0\n",
      "-1  19  84 \n",
      " 0  4   393\n",
      "------------------The report of GB --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.75      0.17      0.28       103\n",
      "           0       0.82      0.98      0.90       397\n",
      "\n",
      "    accuracy                           0.82       500\n",
      "   macro avg       0.79      0.58      0.59       500\n",
      "weighted avg       0.81      0.82      0.77       500\n",
      "\n",
      "    -1    0\n",
      "-1  18  85 \n",
      " 0  6   391\n",
      "------------------The report of RF --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.68      0.27      0.39       103\n",
      "           0       0.84      0.97      0.90       397\n",
      "\n",
      "    accuracy                           0.82       500\n",
      "   macro avg       0.76      0.62      0.64       500\n",
      "weighted avg       0.80      0.82      0.79       500\n",
      "\n",
      "    -1    0\n",
      "-1  28  75 \n",
      " 0  13  384\n",
      "[22:53:03] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { colsample_bytree, gamma, max_depth, min_child_weight, subsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "------------------The report of XGB --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.39      0.40      0.39       103\n",
      "           0       0.84      0.84      0.84       397\n",
      "\n",
      "    accuracy                           0.75       500\n",
      "   macro avg       0.62      0.62      0.62       500\n",
      "weighted avg       0.75      0.75      0.75       500\n",
      "\n",
      "    -1    0\n",
      "-1  41  62 \n",
      " 0  64  333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('KNN',\n",
       "  '              precision    recall  f1-score   support\\n\\n          -1       0.45      0.19      0.27       103\\n           0       0.82      0.94      0.87       397\\n\\n    accuracy                           0.79       500\\n   macro avg       0.64      0.57      0.57       500\\nweighted avg       0.74      0.79      0.75       500\\n'),\n",
       " ('DT',\n",
       "  '              precision    recall  f1-score   support\\n\\n          -1       0.43      0.42      0.42       103\\n           0       0.85      0.85      0.85       397\\n\\n    accuracy                           0.76       500\\n   macro avg       0.64      0.64      0.64       500\\nweighted avg       0.76      0.76      0.76       500\\n'),\n",
       " ('SV',\n",
       "  '              precision    recall  f1-score   support\\n\\n          -1       0.83      0.18      0.30       103\\n           0       0.82      0.99      0.90       397\\n\\n    accuracy                           0.82       500\\n   macro avg       0.82      0.59      0.60       500\\nweighted avg       0.82      0.82      0.78       500\\n'),\n",
       " ('GB',\n",
       "  '              precision    recall  f1-score   support\\n\\n          -1       0.75      0.17      0.28       103\\n           0       0.82      0.98      0.90       397\\n\\n    accuracy                           0.82       500\\n   macro avg       0.79      0.58      0.59       500\\nweighted avg       0.81      0.82      0.77       500\\n'),\n",
       " ('RF',\n",
       "  '              precision    recall  f1-score   support\\n\\n          -1       0.68      0.27      0.39       103\\n           0       0.84      0.97      0.90       397\\n\\n    accuracy                           0.82       500\\n   macro avg       0.76      0.62      0.64       500\\nweighted avg       0.80      0.82      0.79       500\\n'),\n",
       " ('XGB',\n",
       "  '              precision    recall  f1-score   support\\n\\n          -1       0.39      0.40      0.39       103\\n           0       0.84      0.84      0.84       397\\n\\n    accuracy                           0.75       500\\n   macro avg       0.62      0.62      0.62       500\\nweighted avg       0.75      0.75      0.75       500\\n')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitandtrain(list_of_txt,true_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------The report of DT --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.43      0.47      0.45       107\n",
      "           0       0.85      0.83      0.84       393\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.64      0.65      0.65       500\n",
      "weighted avg       0.76      0.76      0.76       500\n",
      "\n",
      "    -1    0\n",
      "-1  50  57 \n",
      " 0  65  328\n",
      "------------------The report of DT --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.36      0.31      0.33       100\n",
      "           0       0.83      0.86      0.85       400\n",
      "\n",
      "    accuracy                           0.75       500\n",
      "   macro avg       0.60      0.59      0.59       500\n",
      "weighted avg       0.74      0.75      0.74       500\n",
      "\n",
      "    -1    0\n",
      "-1  31  69 \n",
      " 0  55  345\n",
      "------------------The report of DT --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.43      0.40      0.41       107\n",
      "           0       0.84      0.85      0.85       393\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.63      0.63      0.63       500\n",
      "weighted avg       0.75      0.76      0.75       500\n",
      "\n",
      "    -1    0\n",
      "-1  43  64 \n",
      " 0  58  335\n",
      "------------------The report of DT --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.38      0.39      0.38       101\n",
      "           0       0.84      0.84      0.84       399\n",
      "\n",
      "    accuracy                           0.75       500\n",
      "   macro avg       0.61      0.61      0.61       500\n",
      "weighted avg       0.75      0.75      0.75       500\n",
      "\n",
      "    -1    0\n",
      "-1  39  62 \n",
      " 0  63  336\n",
      "------------------The report of DT --------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.44      0.39      0.42       113\n",
      "           0       0.83      0.86      0.84       387\n",
      "\n",
      "    accuracy                           0.75       500\n",
      "   macro avg       0.64      0.62      0.63       500\n",
      "weighted avg       0.74      0.75      0.75       500\n",
      "\n",
      "    -1    0\n",
      "-1  44  69 \n",
      " 0  55  332\n"
     ]
    }
   ],
   "source": [
    "reports = []\n",
    "for i in range(5):\n",
    "    reports.append(splitandtrain(list_of_txt,true_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select model parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [4, 5, 6],\n",
    "        'objective': ['binary:logistic','binary:hinge'],\n",
    "        'booster' : ['gbtree','gblinear','dart']\n",
    "        }\n",
    "XGB = xgb.XGBClassifier(learning_rate=0.02, objective='binary:logistic',silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:   28.8s\n",
      "[Parallel(n_jobs=4)]: Done 120 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=4)]: Done 250 out of 250 | elapsed:  3.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:46:48] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=<generator object _BaseKFold.split at 0x000001CBAA65EE40>,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None, gamma=None,\n",
       "                                           gpu_id=None, importance_type='gain',\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=0.02,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weight=None, mis...\n",
       "                                           subsample=None, tree_method=None,\n",
       "                                           validate_parameters=None,\n",
       "                                           verbosity=None),\n",
       "                   n_iter=50, n_jobs=4,\n",
       "                   param_distributions={'booster': ['gbtree', 'gblinear',\n",
       "                                                    'dart'],\n",
       "                                        'colsample_bytree': [0.6, 0.8, 1.0],\n",
       "                                        'gamma': [0.5, 1, 1.5, 2, 5],\n",
       "                                        'max_depth': [4, 5, 6],\n",
       "                                        'min_child_weight': [1, 5, 10],\n",
       "                                        'objective': ['binary:logistic',\n",
       "                                                      'binary:hinge'],\n",
       "                                        'subsample': [0.6, 0.8, 1.0]},\n",
       "                   random_state=1001, scoring='recall', verbose=3)"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds = 5\n",
    "param_comb = 50\n",
    "\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "\n",
    "random_search = RandomizedSearchCV(XGB, param_distributions=params, n_iter=param_comb, scoring='recall', n_jobs=4, cv=skf.split(train_x, train_y), verbose=3, random_state=1001 )\n",
    "\n",
    "random_search.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 294 candidates, totalling 882 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 882 out of 882 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=DecisionTreeClassifier(random_state=42),\n",
       "             param_grid={'max_leaf_nodes': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n",
       "                                            13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
       "                                            22, 23, 24, 25, 26, 27, 28, 29, 30,\n",
       "                                            31, ...],\n",
       "                         'min_samples_split': [2, 3, 4]},\n",
       "             scoring='recall', verbose=1)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter = {'max_leaf_nodes': list(range(2, 100)), 'min_samples_split': [2, 3, 4]}\n",
    "grid_search_cv = GridSearchCV(tree.DecisionTreeClassifier(random_state=42), \n",
    "                              param_grid=parameter, verbose=1, cv=3,scoring='recall')\n",
    "grid_search_cv.fit(xtrain_tfidf_ngram, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3522012578616352"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_cv.best_estimator_\n",
    "grid_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1.0, gamma=0.5, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.02, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=10, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
      "              objective='binary:hinge', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=None, silent=True, subsample=0.6,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "0.1681592039800995\n"
     ]
    }
   ],
   "source": [
    "print(random_search.best_estimator_)\n",
    "print(random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_pickle(r\"C:\\Users\\luoyu\\Desktop\\ise540\\project\\cleaned.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['full_txt'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['full_txt']=data['og_tweet_txt']+data['tweet_txt']\n",
    "full_txt=data['full_txt'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_tfidf_ngram = tfidf_vect_ngram.transform(full_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT.fit(tfidf_vect_ngram.transform(list_of_txt), true_sent)\n",
    "predictions = DT.predict(xtest_tfidf_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-122-09745ea311b4>:3: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  w2v = dict(zip(model.wv.index2word, model.wv.syn0))\n"
     ]
    }
   ],
   "source": [
    "tokens = [nltk.word_tokenize(sentences) for sentences in full_txt]\n",
    "model = gensim.models.Word2Vec(tokens, min_count=1, size=200) \n",
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.update(glove_vectors_w2v)\n",
    "Tfidf_model = TfidfVectorizer()\n",
    "Tfidf_model.fit(full_txt)\n",
    "tf_idf_dict = dict(zip(Tfidf_model.get_feature_names(), list(Tfidf_model.idf_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dembed_train = embedding(list_of_txt)\n",
    "DT.fit(csr_matrix(dembed_train), true_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_full = embedding(full_txt)\n",
    "predictions = DT.predict(csr_matrix(embed_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357474"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=predictions.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 285180, -1: 72294}\n"
     ]
    }
   ],
   "source": [
    "counts = { }          \n",
    "for x in pred:                      \n",
    "    if  x in counts:\n",
    "        counts[x] += 1\n",
    "    else:\n",
    "        counts[x] = 1\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dic=dict(zip(full_txt,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['predict']=predictions\n",
    "data.to_csv('prediction_TFIDF_DT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
